---
title: "Cleaning Data"
format: html
editor: source
---

# Set up

```{r}
#| label: load packages

library(tidyverse)
library(tm)
library(janitor)

```

```{r}
#| label: load data

load("data/trans_news_data.Rda")
```

# Pre-processing

```{r}
#| label: define processing functions

pub_normalize <- function(x) {
  str_to_lower(x) |>
  str_replace_all("[:blank:]", "") |>
  str_replace_all("[:punct:]", "")
}

doctype_normalize <- function(x) {
  str_to_lower(x) |>
  str_replace(".*review.*", "review") |>
  str_replace_all("[:blank:]", "") |>
  str_replace_all(".*obituary.*", "obituary") |>
  str_replace_all("news.+", "news") |>
  str_replace_all("opinion[^s]|opinion$", "opinions") |>
  str_replace_all(".*correction.*", "correctionretraction")
  #str_replace_all("[:punct:]", "") |>
}

fulltext_preprocess <- function (x) {
  str_to_lower(x) |>
  str_replace_all('http\\S+\\s*', "") |>
  str_remove('[:punct:]') |>
  removePunctuation() |>
  removeNumbers() |>
  str_replace_all(' +', ' ') |>
  str_trim(side="both")
}

title_process <- function (x) {
  str_to_lower(x) |>
  str_replace_all("\\[.*\\]", "") |>
  str_replace_all(' +', ' ') |>
  str_trim(side="both") |>
  str_replace_all(":$", "")
} # missing extra space

```

```{r}
clean_df <- doc_df |>
  rename(
    doc_id = proquest_document_id,
    doc_url = document_url
  ) |>
  mutate(
    unproc_full = full_text,
    first_sentence = fulltext_preprocess(str_extract(full_text, ".{100}")),
    # text preprocessing
    title = title_process(title),
    full_text = fulltext_preprocess(full_text),
    source_type = pub_normalize(source_type), #normalize source type
    document_type = doctype_normalize(document_type),
    #normalize factors
    publisher = recode( #recode publisher factor
      pub_normalize(publisher), #apply normalize function to remove spaces and punctuation
      "newyorktimescompany" = "NYT",
      "wpcompanyllcdbathewashingtonpost" = "WP",
      "dowjonescompanyinc" = "WSJ",
      "losangelestimescommunicationsllc" = "LA",
      "tribunepublishingcompanyllc" = "CH"
    ),
    #date
    publication_date = as.Date(str_replace_all(publication_date, " ", ""), format="%b%d,%Y"),#format date
    #solve NAs in date; get date from title get date from copyright
    posted_temp = as.Date(str_remove_all(str_replace_all(str_extract(title, "(posted .*)"), "posted |\\d\\d:\\d\\d:\\d\\d\\).*", ""), " ")),
    cr_temp = as.Date(str_extract(copyright, "[:alpha:]{3} \\d{1,2}, \\d{4}$"), format="%b %d, %Y"),
    posted_temp = coalesce(posted_temp, cr_temp),
    publication_date = coalesce(publication_date, posted_temp),
  ) |>
  filter(
    !publisher %in% c("activision", "brasseysincofdullesva", "newstex", NA), #filter out incorrect publishers
    !document_type %in% c("letter", "correctionretraction", "lettertotheeditor"), #filter out letters
    !rowSums(is.na(doc_df)) == ncol(doc_df), #filter out if row is all NA
    !grepl("^not available|enlarge this image", full_text), #filter out not available full text
    !grepl("^$", title),
    !grepl("calendar|^correction[s$]", title)
  ) |>
  mutate_at(
    c("publisher", "column", "subject", "source_type", "document_type"), as.factor #make factor
  ) |>
  select(
    -contains("_temp"),
    -copyright
  ) |>
  distinct(across(-c(doc_id, doc_url)), .keep_all = TRUE)
```

```{r}
summary(clean_df)
```

```{r}
clean_df <- clean_df |>
  mutate(
    full_text = str_remove_all(full_text, "los angeles times|la times") |>
                str_remove_all("new york times|nyt") |>
                str_remove_all("washington post") |>
                str_remove_all("chicago tribune|tribune") |>
                str_remove_all("wall street journal") |>
                str_remove_all("times|tribune|post|journal") |>
                str_remove_all("new york|los angeles|chicago")
  )
```


# Merge Near Duplicates

```{r}
#dedupe by first sentence
deduped_df <- clean_df |>
  mutate(
    num_na = rowSums(is.na(clean_df))
  ) |>
  arrange(num_na) |>
  distinct(first_sentence, source_type, .keep_all = TRUE) |>
  select(
    -first_sentence
  )
  
```

```{r}
#This removes all of the dupes with title repeated more than 4. This captures all of the 
repeated_dupes <- deduped_df |>
  get_dupes(title) |>
  filter(
    dupe_count >= 4
  )

deduped_df <- deduped_df |>
  anti_join(repeated_dupes, join_by(title))
```


```{r}
web_df <- deduped_df |>
  filter(source_type == "blogpodcastorwebsite") |>
  arrange(num_na)|>
  distinct(title, .keep_all = TRUE)

paper_df <- deduped_df |>
  filter(source_type == "newspaper") |>
  arrange(num_na) |>
  distinct(title, .keep_all = TRUE)
```

```{r}

deduped_df |>
  #filter(publication_date < '2000-01-01') |>
  mutate(publication_year = as.integer(publication_year)) |>
ggplot(aes(x=publication_year, color=source_type)) +
  geom_histogram(stat="count")

```


```{r}
#bind web and paper
wsj_docs <- deduped_df |>
  filter(publisher=="WSJ",
         publication_date >= as.Date('1997-01-01'))

final_df <- web_df |>
  filter(publication_date >= as.Date('1997-01-01')) |>
  bind_rows(
    paper_df |>
      filter(publication_date <= as.Date('1996-12-31'))
  ) |>
  bind_rows(
    wsj_docs
  ) |>
  rowid_to_column("id") #create id
```

```{r}
save(final_df, file="data/trans_news_final_added_wsj.Rda")
```

