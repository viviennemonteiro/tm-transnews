---
title: "Cleaning Data"
format: html
editor: source
---

# Set up

```{r}
#| label: load packages

library(tidyverse)
library(tm)
library(tidytext)
library(janitor)
library(SnowballC)


```

```{r}
#| label: load data

load("data/trans_news_data.Rda")
```

# Pre-processing

```{r}
#| label: define processing functions

pub_normalize <- function(x) {
  str_to_lower(x) |>
  str_replace_all("[:blank:]", "") |>
  str_replace_all("[:punct:]", "")
}

doctype_normalize <- function(x) {
  str_to_lower(x) |>
  str_replace(".*review.*", "review") |>
  str_replace_all("[:blank:]", "") |>
  str_replace_all(".*obituary.*", "obituary") |>
  str_replace_all("news.+", "news") |>
  str_replace_all("opinion[^s]|opinion$", "opinions") |>
  str_replace_all(".*correction.*", "correctionretraction")
  #str_replace_all("[:punct:]", "") |>
}

fulltext_preprocess <- function (x) {
  str_to_lower(x) |>
  str_replace_all('http\\S+\\s*', "") |>
  str_replace_all("\\(?\\d\\d\\d\\)?-\\d\\d\\d-\\d\\d\\d\\d", "") |>
  removePunctuation() |>
  removeNumbers() |>
  str_replace_all(' +', ' ') |>
  str_trim(side="both")
}

title_process <- function (x) {
  str_to_lower(x) |>
  str_replace_all("\\[.*\\]", "") |>
  str_replace_all(' +', ' ') |>
  str_trim(side="both") |>
  str_replace_all(":$", "")
} # missing extra space

```

```{r}
clean_df <- doc_df |>
  rename(
    doc_id = proquest_document_id,
    doc_url = document_url
  ) |>
  mutate(
    first_sentence = fulltext_preprocess(str_extract(full_text, ".{100}")),
    # text preprocessing
    title = title_process(title),
    full_text = fulltext_preprocess(full_text),
    source_type = pub_normalize(source_type), #normalize source type
    document_type = doctype_normalize(document_type),
    #normalize factors
    publisher = recode( #recode publisher factor
      pub_normalize(publisher), #apply normalize function to remove spaces and punctuation
      "newyorktimescompany" = "NYT",
      "wpcompanyllcdbathewashingtonpost" = "WP",
      "dowjonescompanyinc" = "WSJ",
      "losangelestimescommunicationsllc" = "LA",
      "tribunepublishingcompanyllc" = "CH"
    ),
    #date
    publication_date = as.Date(str_replace_all(publication_date, " ", ""), format="%b%d,%Y"),#format date
    #solve NAs in date; get date from title get date from copyright
    posted_temp = as.Date(str_remove_all(str_replace_all(str_extract(title, "(posted .*)"), "posted |\\d\\d:\\d\\d:\\d\\d\\).*", ""), " ")),
    cr_temp = as.Date(str_extract(copyright, "[:alpha:]{3} \\d{1,2}, \\d{4}$"), format="%b %d, %Y"),
    posted_temp = coalesce(posted_temp, cr_temp),
    publication_date = coalesce(publication_date, posted_temp), 
  ) |>
  filter(
    !publisher %in% c("activision", "brasseysincofdullesva", "newstex", NA), #filter out incorrect publishers
    !document_type %in% c("letter", "correctionretraction", "lettertotheeditor"), #filter out letters
    !rowSums(is.na(doc_df)) == ncol(doc_df), #filter out if row is all NA
    !grepl("^not available|enlarge this image", full_text), #filter out not available full text
    !grepl("^$", title),
    !grepl("calendar|^correction[s$]", title)
  ) |>
  mutate_at(
    c("publisher", "column", "subject", "source_type", "document_type"), as.factor #make factor
  ) |>
  select(
    -contains("_temp"),
    -copyright
  ) |>
  distinct(across(-c(doc_id, doc_url)))
```

```{r}
summary(clean_df)
```

# Merge Near Duplicates

```{r}
#dedupe by first sentence
deduped_df <- clean_df |>
  mutate(
    num_na = rowSums(is.na(clean_df))
  ) |>
  arrange(num_na) |>
  distinct(first_sentence, source_type, .keep_all = TRUE) |>
  select(
    -first_sentence
  )
  
```

```{r}
#This removes all of the dupes with title repeated more than 4. This captures all of the 
repeated_dupes <- deduped_df |>
  get_dupes(title) |>
  filter(
    dupe_count >= 4
  )

deduped_df <- deduped_df |>
  anti_join(dupes, join_by(title))
```

## rolling up remaining
```{r}
# roll up differing source type
metadata_rolled <- clean_df |>
  group_by(title, source_type) |>
  summarise(
    across(!full_text,                                  
           ~paste0(unique(na.omit(.x)), collapse = "; "))
  )

```

By title: 7397 - Gets rid of a bunch of briefings - also gets rid of schedules which solves a lot

title ft: 1037 - all the basics leaves some of the slightly differet

title source_type: 4215

```{r}
metadata_rolled |>
  get_dupes(title)
```

# Initial Analysis

```{r}
data("stop_words")

stop_words2 <- stop_words|>
  filter(
    !grepl("^she|^her|^hers|^he$|^he's|^him|^his|^they|^them|^their", word)
  )
```

```{r}
tidy_news <- deduped_df |>
  unnest_tokens(word, full_text)

```

```{r}
tidy_news <- tidy_news |>
  anti_join(stop_words) |>
  mutate(
    stem = case_when(str_detect(word, "[^e|aies$]ies$") ~ str_replace(word, "ies$", "y"),
                    str_detect(word, "[^e|a|oes$]es$") ~ str_replace(word, "es$", "e"),
                    str_detect(word, "[^ss$|us$]s$") ~ str_remove(word, "s$"),
                    TRUE ~ word)
  )
  
```

```{r}
tidy_news |>
  count(word, sort = TRUE)

tidy_news |>
  count(stem, sort = TRUE)
```
